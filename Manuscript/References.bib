
@article{campbell_nga_2008,
	title = {{NGA} {Ground} {Motion} {Model} for the {Geometric} {Mean} {Horizontal} {Component} of {PGA}, {PGV}, {PGD} and 5\% {Damped} {Linear} {Elastic} {Response} {Spectra} for {Periods} {Ranging} from 0.01 to 10 \textit{s}},
	volume = {24},
	issn = {8755-2930, 1944-8201},
	url = {http://journals.sagepub.com/doi/10.1193/1.2857546},
	doi = {10.1193/1.2857546},
	abstract = {We present a new empirical ground motion model for PGA, PGV, PGD and 5\% damped linear elastic response spectra for periods ranging from 0.01–10 s. The model was developed as part of the PEER Next Generation Attenuation (NGA) project. We used a subset of the PEER NGA database for which we excluded recordings and earthquakes that were believed to be inappropriate for estimating free-field ground motions from shallow earthquake mainshocks in active tectonic regimes. We developed relations for both the median and standard deviation of the geometric mean horizontal component of ground motion that we consider to be valid for magnitudes ranging from 4.0 up to 7.5–8.5 (depending on fault mechanism) and distances ranging from 0–200 km. The model explicitly includes the effects of magnitude saturation, magnitude-dependent attenuation, style of faulting, rupture depth, hanging-wall geometry, linear and nonlinear site response, 3-D basin response, and inter-event and intra-event variability. Soil nonlinearity causes the intra-event standard deviation to depend on the amplitude of PGA on reference rock rather than on magnitude, which leads to a decrease in aleatory uncertainty at high levels of ground shaking for sites located on soil.},
	language = {en},
	number = {1},
	urldate = {2024-03-12},
	journal = {Earthquake Spectra},
	author = {Campbell, Kenneth W. and Bozorgnia, Yousef},
	month = feb,
	year = {2008},
	pages = {139--171},
}

@techreport{community_resilience_group_community_2015,
	title = {Community resilience planning guide for buildings and infrastructure systems : volume {II}},
	shorttitle = {Community resilience planning guide for buildings and infrastructure systems},
	url = {https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.1190v2.pdf},
	number = {NIST SP 1190v2},
	urldate = {2023-09-06},
	institution = {National Institute of Standards and Technology},
	author = {{Community Resilience Group}},
	month = oct,
	year = {2015},
	doi = {10.6028/NIST.SP.1190v2},
	pages = {NIST SP 1190v2},
}

@techreport{community_resilience_group_community_2015-1,
	title = {Community resilience planning guide for buildings and infrastructure systems : volume {I}},
	shorttitle = {Community resilience planning guide for buildings and infrastructure systems},
	url = {https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.1190v1.pdf},
	number = {NIST SP 1190v1},
	urldate = {2023-09-06},
	institution = {National Institute of Standards and Technology},
	author = {{Community Resilience Group}},
	month = oct,
	year = {2015},
	doi = {10.6028/NIST.SP.1190v1},
	pages = {NIST SP 1190v1},
}

@misc{mckenna_nheri-simcenterr2dtool_2024,
	title = {{NHERI}-{SimCenter}/{R2DTool}: {Version} 4.0.0},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {{NHERI}-{SimCenter}/{R2DTool}},
	url = {https://zenodo.org/doi/10.5281/zenodo.10448043},
	urldate = {2024-02-22},
	publisher = {Zenodo},
	author = {McKenna, Frank and Gavrilovic, Stevan and Zsarnoczay, Adam and Zhao, Jinyan and Zhong, Kuanshi and Cetiner, Barbaros and Yi, Sang-ri and Elhaddad, Wael and Arduino, Pedro},
	month = jan,
	year = {2024},
	doi = {10.5281/ZENODO.10448043},
}

@article{faiz_risk-averse_2024,
	title = {A risk-averse stochastic optimization model for community resilience planning},
	volume = {92},
	issn = {00380121},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S003801212400034X},
	doi = {10.1016/j.seps.2024.101835},
	language = {en},
	urldate = {2024-02-22},
	journal = {Socio-Economic Planning Sciences},
	author = {Faiz, Tasnim Ibn and Harrison, Kenneth W.},
	month = apr,
	year = {2024},
	pages = {101835},
}

@techreport{harrison_nist_2023,
	address = {Gaithersburg, MD},
	title = {{NIST} {Alternatives} for {Resilient} {Communities} ({NIST} {ARC}) software tool:: mathematical programming model},
	shorttitle = {{NIST} {Alternatives} for {Resilient} {Communities} ({NIST} {ARC}) software tool},
	url = {https://nvlpubs.nist.gov/nistpubs/TechnicalNotes/NIST.TN.2239pt1.pdf},
	abstract = {The National Institute of Standards and Technology (NIST) Alternatives for Resilient Communities (NIST ARC) software is an interactive tool for developing alternative sets of actions that meet community resilience and cost goals, given hazard and interdependency information and socio-economic data.  Community resilience planning is challenging as it involves several large-scale systems and public sector decision-making with numerous stakeholders.  The goal of NIST ARC is to decrease a community s burden in developing viable alternatives for stakeholder consideration.  This technical note details NIST ARC s mathematical programming model, which is the leading technical contribution of NIST ARC.  The model variables, parameters (data), and equations of the two-stage stochastic mixed integer linear programming model are described, with the full model given in the Appendix.  Results for a realistic example designed to test the suitability of the mathematical programming model for supporting interactive community resilience planning are presented.  Finally, the NIST ARC decision support application that enables the use and application of the model, the plans for its further development and testing, and its role within the broader set of NIST-funded tools and guidance for the Community Resilience Program are briefly described.},
	number = {NIST TN 2239pt1},
	urldate = {2024-02-22},
	institution = {National Institute of Standards and Technology (U.S.)},
	author = {Harrison, Kenneth and Faiz, Tasnim Ibn and Farahmandfar, Zeinab and Crawford, Shane and Loerzel, Jarrod},
	month = jan,
	year = {2023},
	doi = {10.6028/NIST.TN.2239pt1},
	pages = {NIST TN 2239pt1},
}

@article{gerstenberger_probabilistic_2020,
	title = {Probabilistic {Seismic} {Hazard} {Analysis} at {Regional} and {National} {Scales}: {State} of the {Art} and {Future} {Challenges}},
	volume = {58},
	issn = {8755-1209, 1944-9208},
	shorttitle = {Probabilistic {Seismic} {Hazard} {Analysis} at {Regional} and {National} {Scales}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1029/2019RG000653},
	doi = {10.1029/2019RG000653},
	abstract = {Seismic hazard modeling is a multidisciplinary science that aims to forecast earthquake occurrence and its resultant ground shaking. Such models consist of a probabilistic framework that quantiﬁes uncertainty across a complex system; typically, this includes at least two model components developed from Earth science: seismic source and ground motion models. Although there is no scientiﬁc prescription for the forecast length, the most common probabilistic seismic hazard analyses consider forecasting windows of 30 to 50 years, which are typically an engineering demand for building code purposes. These types of analyses are the topic of this review paper. Although the core methods and assumptions of seismic hazard modeling have largely remained unchanged for more than 50 years, we review the most recent initiatives, which face the difﬁcult task of meeting both the increasingly sophisticated demands of society and keeping pace with advances in scientiﬁc understanding. A need for more accurate and spatially precise hazard forecasting must be balanced with increased quantiﬁcation of uncertainty and new challenges such as moving from time‐independent hazard to forecasts that are time dependent and speciﬁc to the time period of interest. Meeting these challenges requires the development of science‐driven models, which integrate all information available, the adoption of proper mathematical frameworks to quantify the different types of uncertainties in the hazard model, and the development of a proper testing phase of the model to quantify its consistency and skill. We review the state of the art of the National Seismic Hazard Modeling and how the most innovative approaches try to address future challenges. Plain Language Summary In this review paper we describe the state of the art in modeling earthquake hazard at the national scale. National hazard models take our understanding of fundamental earthquake processes and develop models of earthquake shaking relevant to the decades to come. The shaking estimates from the models provide important inputs into societal decision making across a wide range of uses including such things as building design requirements or for guiding insurance policy. Here were introduce national models from 10 regions around the world, including multinational models that aim to make results comparable from nation to nation. We describe key challenges and assumptions in making the models and provide recommendations about research for improving future generations of national models. An emerging and overriding philosophy is the need to better quantify and make useful the uncertainties in our knowledge of earthquake processes. Future models will better be able to include this uncertainty and will aim to better quantify the ability of the models to provide the outputs society needs. Finally, future models will become increasingly reliant on computer models that simulate how earthquakes interact with each other and cause shaking at the surface of the Earth.},
	language = {English},
	number = {2},
	urldate = {2023-01-13},
	journal = {Reviews of Geophysics},
	author = {Gerstenberger, M. C. and Marzocchi, W. and Allen, T. and Pagani, M. and Adams, J. and Danciu, L. and Field, E. H. and Fujiwara, H. and Luco, N. and Ma, K.‐F. and Meletti, C. and Petersen, M. D.},
	year = {2020},
}

@article{campbell_seismic_2002,
	title = {Seismic hazard model for loss estimation and risk management in {Taiwan}},
	volume = {22},
	issn = {02677261},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0267726102000957},
	doi = {10.1016/S0267-7261(02)00095-7},
	language = {en},
	number = {9-12},
	urldate = {2023-02-02},
	journal = {Soil Dynamics and Earthquake Engineering},
	author = {Campbell, K.W. and Thenhaus, P.C. and Barnhard, T.P. and Hampson, D.B.},
	year = {2002},
	pages = {743--754},
}

@article{tantala_earthquake_2008,
	title = {Earthquake loss estimation for the {New} {York} {City} {Metropolitan} {Region}},
	volume = {28},
	issn = {02677261},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0267726107001364},
	doi = {10.1016/j.soildyn.2007.10.012},
	language = {en},
	number = {10-11},
	urldate = {2023-10-11},
	journal = {Soil Dynamics and Earthquake Engineering},
	author = {Tantala, Michael W. and Nordenson, Guy J.P. and Deodatis, George and Jacob, Klaus},
	month = oct,
	year = {2008},
	pages = {812--835},
}

@article{faccioli_study_1999,
	title = {A study on damage scenarios for residential buildings in {Catania} city},
	volume = {3},
	issn = {13834649},
	url = {http://link.springer.com/10.1023/A:1009856129016},
	doi = {10.1023/A:1009856129016},
	number = {3},
	urldate = {2023-10-11},
	journal = {Journal of Seismology},
	author = {Faccioli, E. and Pessina, V. and Calvi, G.M. and Borzi, B.},
	year = {1999},
	pages = {327--343},
}

@article{cardona_seismic_1997,
	title = {Seismic {Microzonation} and {Estimation} of {Earthquake} {Loss} {Scenarios}: {Integrated} {Risk} {Mitigation} {Project} of {Bogotá}, {Colombia}},
	volume = {13},
	issn = {8755-2930, 1944-8201},
	shorttitle = {Seismic {Microzonation} and {Estimation} of {Earthquake} {Loss} {Scenarios}},
	url = {http://journals.sagepub.com/doi/10.1193/1.1585981},
	doi = {10.1193/1.1585981},
	abstract = {The Universidad de los Andes and the National Institute of Geosciences INGEOMINAS, with the financial support of the national and the municipal governments, have been executing studies to evaluate the seismic hazard and the urban seismic risk for three hypothetical earthquakes that could strike Bogotá, the capital city of Colombia. After having obtained results related to soil amplification using soil dynamic lab studies, analysis of strong-motion records of recent earthquakes and microtremor measurements in a wide area of Bogotá, the study of different scenarios of losses were estimated for different types of buildings and lifelines systems. These earthquake loss estimations have been used by national and local disaster preparedness authorities to design emergency response plans for public information and for educational activities. New requirements are being studied for urban planning, updating the earthquake resistance construction code and for the reinforcement of the seismic rehabilitation of key buildings.},
	language = {en},
	number = {4},
	urldate = {2023-10-11},
	journal = {Earthquake Spectra},
	author = {Cardona, Omar D. and Yamín, Luis E.},
	month = nov,
	year = {1997},
	pages = {795--814},
}

@article{quagliolo_experimental_2021,
	title = {Experimental {Flash} {Floods} {Assessment} {Through} {Urban} {Flood} {Risk} {Mitigation} ({UFRM}) {Model}: {The} {Case} {Study} of {Ligurian} {Coastal} {Cities}},
	volume = {3},
	issn = {2624-9375},
	shorttitle = {Experimental {Flash} {Floods} {Assessment} {Through} {Urban} {Flood} {Risk} {Mitigation} ({UFRM}) {Model}},
	url = {https://www.frontiersin.org/articles/10.3389/frwa.2021.663378/full},
	doi = {10.3389/frwa.2021.663378},
	abstract = {Cities are vulnerable to extreme weather events, particularly by considering flash flood risk as a result of even more short-duration intensive rainfall. In the context of climate change, compound flooding due to simultaneous storm surges and increased runoff may further exacerbate the risk in coastal cities, and it is expected to be frequent and severe across several European urban areas. Despite this increasing evidence, the spatial knowledge of the hazardous events/vulnerabilities through modelling scenarios at the urban level is quite unexplored. Moreover, flood-prone areas often do not correspond to the traditional flood risk classification based on predicted return-period. The result that huge impacts (human losses and damages) occur everywhere throughout the city. Consequently, this new challenge requires stormwater flooding mitigation strategies to adapt to cities while mainstreaming urban flood resilience. In this paper, we considered the Urban Flood Risk Mitigation model through the employment of the open-source tool—Integrated Evaluation of Ecosystem Services and Trade-off (InVEST)—developed by the Natural Capital Project, integrated into a GIS environment. The model application in the three urban coastal territory of the Liguria Region (Italy) estimated the amount of runoff due to two extreme rainfall events for each watershed considered. These index calculation results help define examples of Natural Water Retention Measures (NWRM) per land-use type as resilient solutions by addressing site-specific runoff reduction. Local sensitivity analysis was finally conducted to comprehend the input parameter's influence of rain variation on the model.},
	urldate = {2023-10-11},
	journal = {Frontiers in Water},
	author = {Quagliolo, Carlotta and Comino, Elena and Pezzoli, Alessandro},
	month = may,
	year = {2021},
	pages = {663378},
}

@article{bulti_community_2019,
	title = {Community flood resilience assessment frameworks: a review},
	volume = {1},
	issn = {2523-3963, 2523-3971},
	shorttitle = {Community flood resilience assessment frameworks},
	url = {http://link.springer.com/10.1007/s42452-019-1731-6},
	doi = {10.1007/s42452-019-1731-6},
	language = {en},
	number = {12},
	urldate = {2023-10-11},
	journal = {SN Applied Sciences},
	author = {Bulti, Dejene Tesema and Girma, Birhanu and Megento, Tebarek Lika},
	month = dec,
	year = {2019},
	pages = {1663},
}

@incollection{eslamian_flood_2023,
	address = {Cham},
	title = {Flood and {Drought} {Risk} {Assessment}, {Climate} {Change}, and {Resilience}},
	isbn = {978-3-031-22111-8 978-3-031-22112-5},
	url = {https://link.springer.com/10.1007/978-3-031-22112-5_9},
	language = {en},
	urldate = {2023-10-11},
	booktitle = {Disaster {Risk} {Reduction} for {Resilience}},
	publisher = {Springer International Publishing},
	author = {Cardona, Omar-Darío and Bernal, Gabriel and Escovar, María Alejandra},
	editor = {Eslamian, Saeid and Eslamian, Faezeh},
	year = {2023},
	doi = {10.1007/978-3-031-22112-5_9},
	pages = {191--214},
}

@article{winter_event_2020,
	title = {Event generation for probabilistic flood risk modelling: multi-site peak flow dependence model vs. weather-generator-based approach},
	volume = {20},
	issn = {1684-9981},
	shorttitle = {Event generation for probabilistic flood risk modelling},
	url = {https://nhess.copernicus.org/articles/20/1689/2020/},
	doi = {10.5194/nhess-20-1689-2020},
	abstract = {Abstract. Flood risk assessment is an important prerequisite for risk management decisions. To estimate the risk, i.e. the probability of damage, flood damage needs to be either systematically recorded over a long period or modelled for a series of synthetically generated flood events. Since damage records are typically rare, time series of plausible, spatially coherent event precipitation or peak discharges need to be generated to drive the chain of process models. In the present study, synthetic flood events are generated by two different approaches to modelling flood risk in a meso-scale alpine study area (Vorarlberg, Austria). The first approach is based on the semi-conditional multi-variate dependence model applied to discharge series. The second approach relies on the continuous hydrological modelling of synthetic meteorological fields generated by a multi-site weather generator and using an hourly disaggregation scheme. The results of the two approaches are compared in terms of simulated spatial patterns of peak discharges and overall flood risk estimates. It could be demonstrated that both methods are valid approaches for risk assessment with specific advantages and disadvantages. Both methods are superior to the traditional assumption of a uniform return period, where risk is computed by assuming a homogeneous return period (e.g. 100-year flood) across the entire study area.},
	language = {en},
	number = {6},
	urldate = {2023-10-11},
	journal = {Natural Hazards and Earth System Sciences},
	author = {Winter, Benjamin and Schneeberger, Klaus and Förster, Kristian and Vorogushyn, Sergiy},
	month = jun,
	year = {2020},
	pages = {1689--1703},
}

@misc{noaa_assessing_2023,
	title = {Assessing the {U}.{S}. {Climate} in {August} 2023},
	url = {https://www.ncei.noaa.gov/news/national-climate-202308},
	language = {English},
	journal = {Assessing the U.S. Climate in August 2023},
	author = {NOAA, National Centers for Environmental Information},
	year = {2023},
}

@misc{noaa_billion-dollar_2023,
	title = {Billion-{Dollar} {Weather} and {Climate} {Disasters}},
	url = {https://www.ncei.noaa.gov/access/billions/},
	language = {English},
	journal = {Billion-Dollar Weather and Climate Disasters},
	author = {NOAA, National Centers for Environmental Information},
	year = {2023},
}

@misc{national_institute_of_standard_and_technology_nist_nodate,
	title = {{NIST} {Community} {Resilience} {Overview}},
	url = {https://www.nist.gov/community-resilience},
	language = {English},
	urldate = {2023-09-06},
	author = {National Institute of Standard {and} Technology},
}

@inproceedings{ballantyne_earthquake_1991,
	title = {Earthquake {Loss} {Estimation} {Modeling} of the {Seattle} {Water} {System} {Using} a {Deterministic} {Approach}},
	url = {https://cedb.asce.org/CEDBsearch/record.jsp?dockey=0073945},
	abstract = {This paper summarizes the project and resulting report entitled Earthquake Loss Estimation Modeling of the Seattle Water System (Ballantyne, 1990) conducted under funding from the United States Geological Survey (USGS). The report develops and applies techniques for estimating both direct and indirect earthquake losses to water systems. The water system lifeline is inventoried in a section of metropolitan Seattle. System component losses are estimated as a percent loss or number of pipeline failures per unit length as a function of the severity of ground shaking and liquefaction susceptibility. Component losses are then incorporated into a system network analysis model to assess system operating conditions immediately following an earthquake. Operating conditions are presented in map format as adequate, inadequate, or no water pressure.},
	language = {English},
	urldate = {2022-11-07},
	booktitle = {{ASCE}},
	publisher = {ASCE},
	author = {Ballantyne, Donald B. and Taylor, Craig},
	year = {1991},
	pages = {747--760},
}

@article{parpas_importance_2015,
	title = {Importance {Sampling} in {Stochastic} {Programming}: {A} {Markov} {Chain} {Monte} {Carlo} {Approach}},
	volume = {27},
	issn = {1091-9856},
	abstract = {S tochastic programming models are large-scale optimization problems that are used to facilitate decision making under uncertainty. Optimization algorithms for such problems need to evaluate the expected future costs of current decisions, often referred to as the recourse function. In practice, this calculation is computationally difficult as it requires the evaluation of a multidimensional integral whose integrand is an optimization problem. In turn, the recourse function has to be estimated using techniques such as scenario trees or Monte Carlo methods, both of which require numerous functional evaluations to produce accurate results for large-scale problems with multiple periods and high-dimensional uncertainty. In this work, we introduce an importance sampling framework for stochastic programming that can produce accurate estimates of the recourse function using a small number of samples. Our framework combines Markov chain Monte Carlo methods with kernel density estimation algorithms to build a nonparametric importance sampling distribution, which can then be used to produce a lower-variance estimate of the recourse function. We demonstrate the increased accuracy and efficiency of our approach using variants of well-known multistage stochastic programming problems. Our numerical results show that our framework produces more accurate estimates of the optimal value of stochastic programming models, especially for problems with moderate variance, multimodal, or rare-event distributions.},
	number = {2},
	journal = {INFORMS Journal on Computing},
	author = {Parpas, Panos and Ustun, Berk and Webster, Mort and Tran, Quang Kha},
	year = {2015},
	pages = {358--377},
}

@inproceedings{ball_baltimores_2015,
	title = {Baltimore’s {First} {Step} towards {Advanced} {Pipeline} {Management}},
	isbn = {978-0-7844-7936-0},
	url = {https://ascelibrary.org/doi/10.1061/9780784479360.164},
	doi = {10.1061/9780784479360.164},
	abstract = {Baltimore Metropolitan Water District (BMWD), which provides water to 1.8 million people, decided to undertake a risk-based prioritization for the network of large diameter mains that is the backbone of the system. This includes 86 miles of PCCP. The goal of the prioritization is to provide the BMWD with a transparent and defensible action plan to minimize the risk of catastrophic failure associated with PCCP mains. The prioritization was conducted by grouping pipelines into a logical approach for the overall asset management strategies the BMWD is currently implementing. This desk-top prioritization effort is a planning level tool, based on the current knowledge of the system that will be used to guide the inspection sequence. The resulting model can be easily updated on a semi-annual or annual basis as actual field data is collected and the true knowledge of the system improves. This paper will present the methodologies used to conduct the prioritization and the results of the prioritization demonstrating the importance of a cohesive plan.},
	language = {EN},
	urldate = {2022-11-08},
	booktitle = {Pipelines 2015},
	publisher = {American Society of Civil Engineers},
	author = {Ball, Brian P. and Driscoll, Madeleine and Mazurek, Michael},
	year = {2015},
	pages = {1797--1808},
}

@phdthesis{archibald_infrastructure_2013,
	title = {Infrastructure {Resilience} at the {Project} {Level}: {Considering} {Hazard} {Risk} and {Uncertainty} for {Infrastructure} {Decisions}},
	shorttitle = {Infrastructure {Resilience} at the {Project} {Level}},
	url = {http://rgdoi.net/10.13140/RG.2.1.1040.5368},
	language = {Engli},
	urldate = {2023-01-12},
	school = {University of Delaware},
	author = {Archibald, Erik},
	year = {2013},
	note = {Publisher: Unpublished},
}

@unpublished{woods_managing_2003,
	address = {Institute for Ergonomics, The Ohio State University},
	title = {Managing {Risk} {Proactively}: {The} {Emergence} of {Resilience} {Engineering}},
	abstract = {Research on human reliability, human performance, and organizational aspects of risk have led to the emerging area of Resilience Engineering as an alternative to error tabulations. This paper sketches the development of Resilience Engineering as the new field which enhances organizations’ ability to monitor for risks in how the organization monitors its risks and to target safety investments proactively despite ongoing production and economic pressures.},
	language = {English},
	author = {Woods, David D and Wreathall, John and Wreathall, John},
	year = {2003},
}

@article{lu_improved_1988,
	title = {Improved importance sampling technique for efficient simulation of digital communication systems},
	volume = {6},
	issn = {1558-0008},
	doi = {10.1109/49.192731},
	abstract = {An improved importance sampling technique for the efficient simulation of digital communication systems is proposed. Evaluation of low-probability error events by direct use of classical Monte Carlo (MC) simulation techniques usually involves a very large number of runs. Importance-sampling techniques make the low-probability events occur more frequently. The technique proposed here is based on optimized translations of the original probability densities. The only approximation needed in the optimizations is that of replacing the Q function by the simpler exponential expression. Detailed analytical evaluations of the estimation variances of the classical MC and the conventional and improved importance-sampling approaches for systems with memories and signals are presented and compared, showing the superior performance of the latter. Detailed numerical and simulation results are given.},
	number = {1},
	journal = {IEEE Journal on Selected Areas in Communications},
	author = {Lu, D. and Yao, K.},
	month = jan,
	year = {1988},
	note = {Conference Name: IEEE Journal on Selected Areas in Communications},
	keywords = {Analysis of variance, Computational Intelligence Society, Computational modeling, Digital communication, Discrete event simulation, Error probability, Monte Carlo methods, Radar, Sampling methods, Signal analysis},
	pages = {67--75},
}

@article{younesi_assessing_2020,
	title = {Assessing the resilience of multi microgrid based widespread power systems against natural disasters using {Monte} {Carlo} {Simulation}},
	volume = {207},
	issn = {03605442},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S036054422031327X},
	doi = {10.1016/j.energy.2020.118220},
	abstract = {The primary objective of this paper is to assess the resilience of a large-scale multi-microgrid based power system to cope with the wide-area natural disasters with severe destructive effects. The proposed resilience assessment method is quantitative and reﬂects various aspects of power system such as the fragility and uncertainties along with disaster characteristics such as the type and severity. In addition, it is comparable for different power systems and can be used in decision-making by power system operators and planners for future contingency planning and upgrade schemes. The impact of multiplemicrogrids is entered in the formulations using the calculation of discrete-time multi-state transition model of the power system in response to an extreme event. The tiN À 1me-homogeneous Markov chain is considered to determine the probability of system states (normal, microgrid, and emergency) using the time-independent transition matrix. In order to numerically assess the proposed resilience measure, IEEE 30-bus test case and Iceland 189-bus power system are used and simulations are continued by generating 10000 scenarios considering different event type, severity level and location upon the power system. Finally, Monte Carlo Simulation is used for calculating the resilience metrics.},
	language = {en},
	urldate = {2023-01-17},
	journal = {Energy},
	author = {Younesi, Abdollah and Shayeghi, Hossein and Safari, Amin and Siano, Pierluigi},
	month = sep,
	year = {2020},
	pages = {118220},
}

@article{tabandeh_uncertainty_2022,
	title = {Uncertainty propagation in risk and resilience analysis of hierarchical systems},
	volume = {219},
	issn = {09518320},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0951832021006876},
	doi = {10.1016/j.ress.2021.108208},
	abstract = {A novel formulation is proposed for uncertainty propagation in risk and resilience analysis of hierarchical systems. The main challenges are related to the complexity of hierarchical systems’ computational workflow and high-dimensional probability space. The computational workflow in regional risk and resilience analysis consists of many interconnected sub-models to predict future hazards, the reliability and functionality of physical systems, and the recovery of disrupted services. The complexity of the computational workflow limits the number of model evaluations for uncertainty propagation. In contrast, the computational workflow contains many sources of uncertainty that demand extensive model evaluations to accurately estimate their effects. The proposed formulation in this paper consists of a multi-level uncertainty propagation approach to reduce the problem dimensionality and a variables-grouping approach to reduce the number of model evaluations. The idea of the multi-level uncertainty propagation is to break down the high-dimensional problem into several low-dimensional ones, one for each level of the hierarchy in the computational workflow. The proposed variables-grouping approach provides an adaptive refinement of uncertainty propagation to identify the influential uncertain input data and computational sub-models. The paper illustrates the proposed formulation through a well-known academic problem and regional risk and resilience analysis of a community.},
	language = {en},
	urldate = {2023-01-17},
	journal = {Reliability Engineering \& System Safety},
	author = {Tabandeh, Armin and Sharma, Neetesh and Gardoni, Paolo},
	month = mar,
	year = {2022},
	pages = {108208},
}

@article{cicilio_electrical_2020,
	title = {Electrical grid resilience framework with uncertainty},
	volume = {189},
	issn = {03787796},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0378779620306040},
	doi = {10.1016/j.epsr.2020.106801},
	abstract = {Resilience in our electrical grid is imperative to the well being of society after events such as natural disasters or cyber attacks. To justify the development of resilience improvements in the grid, metrics are needed to quantify the improvement and cost beneﬁt. These resilience metrics need to consider the inherent uncertainty in the grid, which arise from elements such as variable load and generation. This paper presents a method to include uncertainty in the proposed resilience framework. The resilience framework is demonstrated on a 2000-bus synthetic grid with a transient contingency simulated as a hurricane type event with numerous line outages. Varying amounts of distributed energy resources (DERs) at 0\%, 10\%, 20\%, and 50\% of load amount are included in the system and assessed for their system resilience impact and cost-beneﬁt with and without uncertainty evaluated using Monte Carlo simulation.},
	language = {en},
	urldate = {2023-01-17},
	journal = {Electric Power Systems Research},
	author = {Cicilio, P. and Swartz, L. and Vaagensmith, B. and Rieger, C. and Gentle, J. and McJunkin, T. and Cotilla-Sanchez, E.},
	month = dec,
	year = {2020},
	pages = {106801},
}

@article{zheng_bayesian-based_2022,
	title = {Bayesian-based seismic resilience assessment for high-rise buildings with the uncertainty in various variables},
	volume = {51},
	issn = {23527102},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2352710222003345},
	doi = {10.1016/j.jobe.2022.104321},
	abstract = {This paper presents a probabilistic methodology that can consider the uncertainty coming from various random variables to assess seismic resilience for high-rise buildings. A 42-story composite steel frame-RC core tube building is used to show the proposed method. Uncertainties associated with the input loads, annual discount rate, cost ratio, recovery time and unknown parameters (or coefficients) in the demand model for fragility analysis are taken into consideration. Firstly, the Bayesian updating rule is applied to determine the posterior distribution of unknown parameters in the demand model. Then, the epistemic uncertainty of unknown model parameters is considered according to the full probability theory, which is the main novelty of this study. Additionally, the record-to-record method is adopted to consider the uncertainty associated with input seismic ground motions. Furthermore, the Monte-Carlo simulation is used to generate samples of the annual discount rate, cost ratio and recovery time accounting for the impacts of their uncertainties on the structural functionality loss and seismic resilience. It can be concluded that the epistemic uncertainty associated with unknown parameters in the demand model has significant impacts on the fragility estimates and seismic resilience. This study provides an effective way to incorporate the uncertainty of various random variables into the seismic resil­ ience assessment, which is helpful for the decision-makers to implement appropriate mitigation strategies.},
	language = {en},
	urldate = {2023-01-17},
	journal = {Journal of Building Engineering},
	author = {Zheng, Xiao-Wei and Li, Hong-Nan and Lv, Heng-Lin and Huo, Lin-Sheng and Zhang, Ying-Ying},
	month = jul,
	year = {2022},
	pages = {104321},
}

@article{aslam_ansari_data-driven_2020,
	title = {Data-driven {Operation} {Risk} {Assessment} of {Wind}-integrated {Power} {Systems} via {Mixture} {Models} and {Importance} {Sampling}},
	volume = {8},
	issn = {2196-5625},
	url = {https://ieeexplore.ieee.org/document/9086995},
	doi = {10.35833/MPCE.2019.000163},
	abstract = {The increasing penetration of highly intermittent wind generation could seriously jeopardize the operation reliability of power systems and increase the risk of electricity outages. To this end, this paper proposes a novel data-driven method for operation risk assessment of wind-integrated power systems. Firstly, a new approach is presented to model the uncertainty of wind power in lead time. The proposed approach employs k-means clustering and mixture models (MMs) to construct time-dependent probability distributions of wind power. The proposed approach can also capture the complicated statistical features of wind power such as multimodality. Then, a nonsequential Monte Carlo simulation (NSMCS) technique is adopted to evaluate the operation risk indices. To improve the computation performance of NSMCS, a cross-entropy based importance sampling (CE-IS) technique is applied. The CE-IS technique is modified to include the proposed model of wind power. The method is validated on a modified IEEE 24-bus reliability test system (RTS) and a modified IEEE 3-area RTS while employing the historical data of wind generation. The simulation results verify the importance of accurate modeling of shortterm uncertainty of wind power for operation risk assessment. Further case studies have been performed to analyze the impact of transmission systems on operation risk indices. The computational performance of the framework is also examined.},
	language = {en},
	number = {3},
	urldate = {2023-01-17},
	journal = {Journal of Modern Power Systems and Clean Energy},
	author = {Aslam Ansari, Osama and Gong, Yuzhong and Liu, Weijia and Yung Chung, Chi},
	year = {2020},
	pages = {437--445},
}

@article{chaudhuri_information_2020,
	title = {Information {Reuse} for {Importance} {Sampling} in {Reliability}-{Based} {Design} {Optimization}},
	volume = {201},
	issn = {09518320},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0951832019301620},
	doi = {10.1016/j.ress.2020.106853},
	abstract = {This paper introduces a new approach for importance-sampling-based reliability-based design optimization (RBDO) that reuses information from past optimization iterations to reduce computational effort. RBDO is a twoloop process—an uncertainty quantification loop embedded within an optimization loop—that can be computationally prohibitive due to the numerous evaluations of expensive high-fidelity models to estimate the probability of failure in each optimization iteration. In this work, we use the existing information from past optimization iterations to create efficient biasing densities for importance sampling estimates of probability of failure. The method involves two levels of information reuse: (1) reusing the current batch of samples to construct an a posteriori biasing density with optimal parameters, and (2) reusing the a posteriori biasing densities of the designs visited in past optimization iterations to construct the biasing density for the current design. We demonstrate for the RBDO of a benchmark speed reducer problem and a combustion engine problem that the proposed method leads to computational savings in the range of 51\% to 76\%, compared to building biasing densities with no reuse in each iteration.},
	language = {en},
	urldate = {2023-01-17},
	journal = {Reliability Engineering \& System Safety},
	author = {Chaudhuri, Anirban and Kramer, Boris and Willcox, Karen E.},
	month = sep,
	year = {2020},
	pages = {106853},
}

@article{ching_efficient_2009,
	title = {Efficient {Evaluation} of {Reliability} for {Slopes} with {Circular} {Slip} {Surfaces} {Using} {Importance} {Sampling}},
	volume = {135},
	issn = {1090-0241, 1943-5606},
	url = {https://ascelibrary.org/doi/10.1061/%28ASCE%29GT.1943-5606.0000035},
	doi = {10.1061/(ASCE)GT.1943-5606.0000035},
	language = {en},
	number = {6},
	urldate = {2023-01-17},
	journal = {Journal of Geotechnical and Geoenvironmental Engineering},
	author = {Ching, Jianye and Phoon, Kok-Kwang and Hu, Yu-Gang},
	month = jun,
	year = {2009},
	pages = {768--777},
}

@article{depina_coupling_2017,
	title = {Coupling the cross-entropy with the line sampling method for risk-based design optimization},
	volume = {55},
	issn = {1615-147X, 1615-1488},
	url = {http://link.springer.com/10.1007/s00158-016-1596-x},
	doi = {10.1007/s00158-016-1596-x},
	language = {en},
	number = {5},
	urldate = {2023-01-17},
	journal = {Structural and Multidisciplinary Optimization},
	author = {Depina, Ivan and Papaioannou, Iason and Straub, Daniel and Eiksund, Gudmund},
	month = may,
	year = {2017},
	pages = {1589--1612},
}

@article{tomasson_improved_2017,
	title = {Improved {Importance} {Sampling} for {Reliability} {Evaluation} of {Composite} {Power} {Systems}},
	volume = {32},
	issn = {0885-8950, 1558-0679},
	url = {http://ieeexplore.ieee.org/document/7581049/},
	doi = {10.1109/TPWRS.2016.2614831},
	abstract = {This paper presents an improved way of applying Monte Carlo simulation using the crossentropy method to calculate the risk of capacity deﬁcit of a composite power system. By applying importance sampling for load states in addition to the generation and transmission states in a systematic manner, the proposed method is many orders of magnitude more efﬁcient than the crude Monte Carlo simulation and considerably more efﬁcient than other crossentropy-based algorithms that apply other ways of estimating the importance sampling distributions. An effective performance metric of system states is applied in order to ﬁnd optimal importance sampling distributions during presimulation that signiﬁcantly reduces the required computational effort. Simulations, using well-known IEEE reliability test systems, show that even problems that are nearly intractable using crude Monte Carlo simulation become very manageable using the proposed method.},
	language = {en},
	number = {3},
	urldate = {2023-01-17},
	journal = {IEEE Transactions on Power Systems},
	author = {Tomasson, Egill and Soder, Lennart},
	month = may,
	year = {2017},
	pages = {2426--2434},
}

@article{heinkenschloss_conditional-value-at-risk_2018,
	title = {Conditional-{Value}-at-{Risk} {Estimation} via {Reduced}-{Order} {Models}},
	volume = {6},
	issn = {2166-2525},
	url = {https://epubs.siam.org/doi/10.1137/17M1160069},
	doi = {10.1137/17M1160069},
	abstract = {This paper proposes and analyzes two reduced-order model (ROM) based approaches for the eﬃcient and accurate evaluation of the Conditional-Value-at-Risk (CVaR) of quantities of interest (QoI) in engineering systems with uncertain parameters. CVaR is used to model objective or constraint functions in risk-averse engineering design and optimization applications under uncertainty. Evaluating the CVaR of the QoI requires sampling in the tail of the QoI distribution and typically requires many solutions of an expensive full-order model of the engineering system. Our ROM approaches substantially reduce this computational expense. Both ROM-based approaches use Monte Carlo (MC) sampling. The ﬁrst approach replaces the computationally expensive full-order model by an inexpensive ROM. The resulting CVaR estimation error is proportional to the ROM error in the so-called risk region, a small region in the space of uncertain system inputs. The second approach uses a combination of full-order model and ROM evaluations via importance sampling and is eﬀective even if the ROM has large errors. In the importance sampling approach, ROM samples are used to estimate the risk region and to construct a biasing distribution. A few full-order model samples are then drawn from this biasing distribution. Asymptotically, as the ROM error goes to zero, the importance sampling estimator reduces the variance by a factor of 1 − β 1, where β ∈ (0, 1) is the quantile level at which CVaR is computed. Numerical experiments on a system of semilinear convection-diﬀusion-reaction equations illustrate the performance of the approaches.},
	language = {en},
	number = {4},
	urldate = {2023-01-17},
	journal = {SIAM/ASA Journal on Uncertainty Quantification},
	author = {Heinkenschloss, Matthias and Kramer, Boris and Takhtaganov, Timur and Willcox, Karen},
	month = jan,
	year = {2018},
	pages = {1395--1423},
}

@article{rockafellar_buffered_2010,
	title = {On buffered failure probability in design and optimization of structures},
	volume = {95},
	issn = {09518320},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0951832010000177},
	doi = {10.1016/j.ress.2010.01.001},
	abstract = {In reliability engineering focused on the design and optimization of structures, the typical measure of reliability is the probability of failure of the structure or its individual components relative to speciﬁc limit states. However, the failure probability has troublesome properties that raise several theoretical, practical, and computational issues. This paper explains the seriousness of these issues in the context of design optimization and goes on to propose a new alternative measure, the buffered failure probability, which offers signiﬁcant advantages. The buffered failure probability is handled with relative ease in design optimization problems, accounts for the degree of violation of a performance threshold, and is more conservative than the failure probability.},
	language = {en},
	number = {5},
	urldate = {2023-01-17},
	journal = {Reliability Engineering \& System Safety},
	author = {Rockafellar, R.T. and Royset, J.O.},
	month = may,
	year = {2010},
	pages = {499--510},
}

@article{melchers_importance_1989,
	title = {Importance sampling in structural systems},
	volume = {6},
	issn = {0167-4730},
	url = {https://www.sciencedirect.com/science/article/pii/0167473089900039},
	doi = {10.1016/0167-4730(89)90003-9},
	abstract = {Importance sampling as a technique to improve the Monte Carlo method for probability integration can be shown to be extremely efficient and versatile. This paper addresses the accuracy and efficiency of the method, and its application to series and parallel systems in structures.},
	language = {en},
	number = {1},
	urldate = {2023-01-17},
	journal = {Structural Safety},
	author = {Melchers, R. E.},
	month = jul,
	year = {1989},
	keywords = {Monte Carlo, failure probability, importance sampling, integration, parallel systems, reliability, series systems, simulation techniques, systems reliability},
	pages = {3--10},
}

@article{wilson_rail_2005,
	title = {Rail human factors: {Past}, present and future},
	volume = {36},
	issn = {00036870},
	shorttitle = {Rail human factors},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0003687005001158},
	doi = {10.1016/j.apergo.2005.07.001},
	abstract = {Rail human factors research has grown rapidly in both quantity and quality of output over the past few years. There was an early base of work at a few institutions carried out over the 1960s and 1970s, followed by a lull in the 1980s and early 1990s. The continual inﬂuences of safety concerns, new technical system opportunities, reorganisation of the business, needs to increase effective, reliable and safe use of capacity, and increased society, media and government interest have now accelerated rail human factors research programmes in several countries. In this paper we review the literature on rail human factors research, covering driving, signalling and control, maintenance, reporting systems, passenger interests, planning and technical systems change. Current major rail human factors programmes are summarised and future research needs proposed. It is asserted that general human factors models and methods are being re-assessed, and new ones developed, to meet the requirements of the railways.},
	language = {en},
	number = {6},
	urldate = {2023-01-12},
	journal = {Applied Ergonomics},
	author = {Wilson, John R. and Norris, Beverley J.},
	month = nov,
	year = {2005},
	pages = {649--660},
}

@incollection{gardoni_life-cycle_2017,
	address = {Cham},
	title = {Life-{Cycle} {Analysis} of {Engineering} {Systems}: {Modeling} {Deterioration}, {Instantaneous} {Reliability}, and {Resilience}},
	isbn = {978-3-319-52424-5 978-3-319-52425-2},
	shorttitle = {Life-{Cycle} {Analysis} of {Engineering} {Systems}},
	url = {http://link.springer.com/10.1007/978-3-319-52425-2_20},
	abstract = {This chapter proposes a novel general stochastic formulation for the Life-Cycle Analysis (LCA) of deteriorating engineering systems. The formulation rigorously formalizes the diﬀerent aspect of the life-cycle of engineering systems. To capture the probabilistic nature of the proposed formulation, it is named Stochastic Life-Cycle Analysis (SLCA). The life-cycle of an engineering system is shaped by deterioration processes and repair/recovery processes both characterized by several sources of uncertainty. The deterioration might be due to exposure to environmental conditions and to both routine and extreme loading. The repair and recovery strategies are typically implemented to restore or enhance the safety and functionality of the engineering system. In the SLCA, state-dependent stochastic models are proposed to capture the impact of deterioration processes and repair/recovery strategies on the engineering systems in terms of performance measures like instantaneous reliability and resilience. The formulation integrates the state-dependent stochastic models with the previously developed Renewal Theory-based Life-Cycle Analysis (RTLCA) to eﬃciently evaluate additional system performance measures such as availability, operation cost and beneﬁts. The proposed SLCA can be used for the optimization of the initial design and mitigation strategies of engineering systems accounting for their life-cycle performance. As an illustration, the proposed SLCA is used to model the life-cycle of a reinforced concrete bridge, subject to deteriorations caused by corrosion and earthquake excitations. The deteriorated bridge column is repaired using Fiber Reinforced Polymer (FRP) composites. The results show that the deterioration processes signiﬁcantly aﬀect the performance measures of the example bridge.},
	language = {en},
	urldate = {2023-01-12},
	booktitle = {Risk and {Reliability} {Analysis}: {Theory} and {Applications}},
	publisher = {Springer International Publishing},
	author = {Jia, Gaofeng and Tabandeh, Armin and Gardoni, Paolo},
	editor = {Gardoni, Paolo},
	year = {2017},
	doi = {10.1007/978-3-319-52425-2_20},
	note = {Series Title: Springer Series in Reliability Engineering},
	pages = {465--494},
}

@article{padgett_sensitivity_2007,
	title = {Sensitivity of {Seismic} {Response} and {Fragility} to {Parameter} {Uncertainty}},
	volume = {133},
	issn = {0733-9445, 1943-541X},
	url = {https://ascelibrary.org/doi/10.1061/%28ASCE%290733-9445%282007%29133%3A12%281710%29},
	doi = {10.1061/(ASCE)0733-9445(2007)133:12(1710)},
	abstract = {As the use for regional seismic risk assessment increases, the need for reliable fragility curves for portfolios ͑or classes͒ of structures becomes more important. Fragility curves for portfolios of structures have the added complexity of having to deal with the uncertainty in geometric properties, along with the typical uncertainties such as material or component response parameters. Analysts are challenged with selecting a prudent level of uncertainty treatment while balancing the simulation and computational effort. In order to address this question, this study ﬁrst evaluates the modeling parameters which signiﬁcantly affect the seismic response of an example class of retroﬁtted bridges. Further, the relative importance of the uncertainty in these modeling parameters, gross geometries, and ground motions is assessed. The study reveals that savings in simulation and computational effort in fragility estimation may be achieved through a preliminary screening of modeling parameters. However, the propagation of these potentially variable parameters tends to be overshadowed by the uncertainty in the ground motion and base geometry of the structural class.},
	language = {en},
	number = {12},
	urldate = {2023-01-12},
	journal = {Journal of Structural Engineering},
	author = {Padgett, Jamie Ellen and DesRoches, Reginald},
	month = dec,
	year = {2007},
	pages = {1710--1718},
}

@article{allen_adaptive_2011,
	title = {Adaptive management for a turbulent future},
	volume = {92},
	issn = {03014797},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0301479710004226},
	doi = {10.1016/j.jenvman.2010.11.019},
	abstract = {The challenges that face humanity today differ from the past because as the scale of human inﬂuence has increased, our biggest challenges have become global in nature, and formerly local problems that could be addressed by shifting populations or switching resources, now aggregate (i.e., “scale up”) limiting potential management options. Adaptive management is an approach to natural resource management that emphasizes learning through management based on the philosophy that knowledge is incomplete and much of what we think we know is actually wrong. Adaptive management has explicit structure, including careful elucidation of goals, identiﬁcation of alternative management objectives and hypotheses of causation, and procedures for the collection of data followed by evaluation and reiteration. It is evident that adaptive management has matured, but it has also reached a crossroads. Practitioners and scientists have developed adaptive management and structured decision making techniques, and mathematicians have developed methods to reduce the uncertainties encountered in resource management, yet there continues to be misapplication of the method and misunderstanding of its purpose. Ironically, the confusion over the term “adaptive management” may stem from the ﬂexibility inherent in the approach, which has resulted in multiple interpretations of “adaptive management” that fall along a continuum of complexity and a priori design. Adaptive management is not a panacea for the navigation of ‘wicked problems’ as it does not produce easy answers, and is only appropriate in a subset of natural resource management problems where both uncertainty and controllability are high. Nonetheless, the conceptual underpinnings of adaptive management are simple; there will always be inherent uncertainty and unpredictability in the dynamics and behavior of complex social-ecological systems, but management decisions must still be made, and whenever possible, we should incorporate learning into management.},
	language = {en},
	number = {5},
	urldate = {2023-01-12},
	journal = {Journal of Environmental Management},
	author = {Allen, Craig R. and Fontaine, Joseph J. and Pope, Kevin L. and Garmestani, Ahjond S.},
	month = may,
	year = {2011},
	pages = {1339--1345},
}

@techreport{noauthor_city_2021,
	title = {City of {Charleston}},
	url = {chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://www.charleston-sc.gov/DocumentCenter/View/31237/FAST-FACTS-2021-revised},
	year = {2021},
}

@article{alperovits_design_1977,
	title = {Design of optimal water distribution systems},
	volume = {13},
	issn = {00431397},
	url = {http://doi.wiley.com/10.1029/WR013i006p00885},
	doi = {10.1029/WR013i006p00885},
	language = {en},
	number = {6},
	urldate = {2022-12-06},
	journal = {Water Resources Research},
	author = {Alperovits, E. and Shamir, U.},
	month = dec,
	year = {1977},
	pages = {885--900},
}

@book{asmussen_stochastic_2007,
	title = {Stochastic {Simulation}: {Algorithms} and {Analysis}},
	volume = {57},
	shorttitle = {Stochastic {Simulation}},
	url = {https://link.springer.com/book/10.1007/978-0-387-69033-9},
	language = {en},
	urldate = {2022-12-02},
	publisher = {New York: Springer},
	author = {Asmussen, S and Glynn, P. W},
	year = {2007},
}

@techreport{klise_water_2017,
	title = {Water {Network} {Tool} for {Resilience} ({WNTR}) {User} {Manual}},
	url = {https://www.osti.gov/servlets/purl/1376816/},
	language = {en},
	number = {SAND2017--8883R, 1376816, 656369},
	urldate = {2022-12-01},
	author = {Klise, Katherine and Hart, David and Moriarty, Dylan and Bynum, Michael and Murray, Regan and Burkhardt, Jonathan and Haxton, Terra},
	month = aug,
	year = {2017},
	doi = {10.2172/1376816},
	pages = {SAND2017--8883R, 1376816, 656369},
}

@article{alpak_techniques_2013,
	title = {Techniques for effective simulation, optimization, and uncertainty quantification of the in-situ upgrading process},
	volume = {3-4},
	issn = {2213-3976},
	url = {https://www.sciencedirect.com/science/article/pii/S2213397613000153},
	doi = {10.1016/j.juogr.2013.09.001},
	abstract = {Strongly temperature-dependent compositional flow/transport, chemical reactions, delivery of energy into the subsurface through downhole heaters, and complex natural fracture architecture render the dynamic modeling of in-situ upgrading process (IUP) a computationally challenging endeavor for carbonate extra-heavy-oil resources. Economic-performance indicators for IUP can be considerably enhanced via pattern optimization. IUP is endowed with uncertain subsurface parameters as in the case of other recovery mechanisms. Simulation results must reflect the impacts of these uncertainties; hence they should always deliver “expected-value production functions” and their attached uncertainty ranges, in short, the “error bars”. Both the optimization and uncertainty quantification workflows require (typically multiple) multi-scenario simulations, and are therefore very compute intensive. We describe our recent developments in simulation techniques, optimization algorithms, tool capabilities, and high-performance computing protocols that in unison form a massively parallel simulation/optimization/uncertainty–quantification workflow, in which it is almost equally easy to produce recovery time-functions with an attached uncertainty range, as it is to run a single simulation. Our simulation platform supports various optimization and uncertainty quantification methods, such as conventional as well as robust optimization using a novel simultaneous perturbation and multivariate interpolation technique, experimental design, and Monte Carlo simulation, that can be linked together through a unified script-based interface, to carry out optimization in the presence of subsurface uncertainties and to quantify the impact of these uncertainties on simulation results. Application of our massively parallel dynamic modeling workflow is illustrated on a proprietary IUP recovery method for a complex naturally fractured extra-heavy oil (bitumen) reservoir as example. After briefly explaining these recovery processes and the modeling approach, we show the techniques (including their accompanying application results) that notably accelerate the (single-model) simulation process; effectively identify the predominant subsurface uncertainties; rapidly optimize heater-producer patterns under the influence of predominant subsurface uncertainties; and efficiently compute expected-value production functions with error bars.},
	language = {en},
	urldate = {2022-11-21},
	journal = {Journal of Unconventional Oil and Gas Resources},
	author = {Alpak, Faruk O. and Vink, Jeroen C. and Gao, Guohua and Mo, Weijian},
	month = dec,
	year = {2013},
	keywords = {Dual-permeability, Experimental design, Extra-heavy oil, In-situ upgrading process, Optimization, Uncertainty quantification},
	pages = {1--14},
}

@article{adhikari_experimental_2009,
	title = {Experimental case studies for uncertainty quantification in structural dynamics},
	volume = {24},
	issn = {0266-8920},
	url = {https://www.sciencedirect.com/science/article/pii/S0266892009000125},
	doi = {10.1016/j.probengmech.2009.01.005},
	abstract = {The consideration of uncertainties in numerical models to obtain the probabilistic descriptions of vibration response is becoming more desirable for industrial-scale finite element models. Broadly speaking, there are two aspects to this problem. The first is the quantification of parametric and non-parametric uncertainties associated with the model and the second is the propagation of uncertainties through the model. While the methods of uncertainty propagation have been extensively researched in the past three decades (e.g., the stochastic finite element method), only relatively recently has quantification been considered seriously. This paper considers uncertainty quantification with the aim of gaining more insight into the nature of uncertainties in medium- and high-frequency vibration problems. This paper describes the setup and results from two experimental studies that may be used for this purpose. The first experimental work described in this paper uses a fixed–fixed beam with 12 masses placed at random locations. The total ‘random mass’ is about 2\% of the total mass of the beam and this experiment simulates ‘random errors’ in the mass matrix. The second experiment involves a cantilever plate with 10 randomly placed spring-mass oscillators. The oscillating mass of each of the 10 oscillators is about 1\% of the mass of the plate. One hundred nominally identical dynamical systems are created and individually tested for each experiment. The probabilistic characteristics of the frequency response functions are discussed in the low, medium and high frequency ranges. The variability in the amplitude of the measured frequency response functions is compared with numerical Monte Carlo simulation results. The data obtained in these experiments may be useful for the validation of uncertainty quantification and propagation methods in structural dynamics.},
	language = {en},
	number = {4},
	urldate = {2022-11-21},
	journal = {Probabilistic Engineering Mechanics},
	author = {Adhikari, S. and Friswell, M. I. and Lonkar, K. and Sarkar, A.},
	month = oct,
	year = {2009},
	keywords = {Beam experiment, Experimental modal analysis, Model validation, Stochastic dynamical systems, Uncertainty quantification},
	pages = {473--492},
}

@book{dror_modeling_2002,
	title = {Modeling {Uncertainty}: {An} {Examination} of {Stochastic} {Theory}, {Methods}, and {Applications}},
	isbn = {978-0-7923-7463-3},
	shorttitle = {Modeling {Uncertainty}},
	abstract = {Modeling Uncertainty: An Examination of Stochastic Theory, Methods, and Applications, is a volume undertaken by the friends and colleagues of Sid Yakowitz in his honor. Fifty internationally known scholars have collectively contributed 30 papers on modeling uncertainty to this volume. Each of these papers was carefully reviewed and in the majority of cases the original submission was revised before being accepted for publication in the book. The papers cover a great variety of topics in probability, statistics, economics, stochastic optimization, control theory, regression analysis, simulation, stochastic programming, Markov decision process, application in the HIV context, and others. There are papers with a theoretical emphasis and others that focus on applications. A number of papers survey the work in a particular area and in a few papers the authors present their personal view of a topic. It is a book with a considerable number of expository articles, which are accessible to a nonexpert - a graduate student in mathematics, statistics, engineering, and economics departments, or just anyone with some mathematical background who is interested in a preliminary exposition of a particular topic. Many of the papers present the state of the art of a specific area or represent original contributions which advance the present state of knowledge. In sum, it is a book of considerable interest to a broad range of academic researchers and students of stochastic systems.},
	language = {en},
	publisher = {Springer Science \& Business Media},
	author = {Dror, Moshe and L'Ecuyer, Pierre and Szidarovszky, Ferenc},
	month = jan,
	year = {2002},
	note = {Google-Books-ID: aX\_f45cO5rsC},
	keywords = {Business \& Economics / Operations Research, Mathematics / Probability \& Statistics / General, Mathematics / Probability \& Statistics / Stochastic Processes, Technology \& Engineering / Operations Research},
}

@article{choe_seismic_2009,
	title = {Seismic fragility estimates for reinforced concrete bridges subject to corrosion},
	volume = {31},
	issn = {01674730},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167473008000982},
	doi = {10.1016/j.strusafe.2008.10.001},
	abstract = {The paper develops novel probabilistic models for the seismic demand of reinforced concrete bridges subject to corrosion. The models are developed by extending currently available probabilistic models for pristine bridges with a probabilistic model for time-dependent chloride-induced corrosion. In particular, the models are developed for deformation and shear force demands. The demand models are combined with existing capacity models to obtain seismic fragility estimates of bridges during their service life. The estimates are applicable to bridges with different combinations of chloride exposure condition, environmental oxygen availability, water-to-cement ratios, and curing conditions. Model uncertainties in the demand, capacity and corrosion models are accounted for, in addition to the uncertainties in the environmental conditions, material properties, and structural geometry. As an application, the fragility of a single-bent bridge typical of current California practice is presented to demonstrate the developed methodology. Sensitivity and importance analyses are conducted to identify the parameters that contribute most to the reliability of the bridge and the random variables that have the largest effect on the variance of the limit state functions and thus are most important sources of uncertainty.},
	language = {en},
	number = {4},
	urldate = {2022-11-21},
	journal = {Structural Safety},
	author = {Choe, Do-Eun and Gardoni, Paolo and Rosowsky, David and Haukaas, Terje},
	month = jul,
	year = {2009},
	pages = {275--283},
}

@article{javanbarg_seismic_nodate,
	title = {Seismic reliability assessment of water supply systems},
	abstract = {This study develops a comprehensive seismic reliability model for serviceability assessment of water supply systems. The model accounts for simulating both pipe leakage and breakage as the damage states. A probabilistic model of leakage simulation is applied to estimate the leakage rate of the pipelines under seismic condition. A Monte Carlo simulation is then performed to probabilistically simulate damage in system. The hydraulic analysis of the damage simulated network is performed and serviceability measure is calculated as the final result. Efficiency of the proposed model is examined by applying it to seismic performance simulation of Osaka City water distribution network due to the 1995 Kobe Earthquake. The results of simulation model are validated with the actual observation during the event. The results of this study suggest that the proposed seismic reliability model can be a useful tool for assessing the seismic performance of water supply systems.},
	language = {en},
	author = {Javanbarg, M B and Takada, S},
	pages = {9},
}

@article{han_probabilistic_2012,
	title = {Probabilistic seismic hazard analysis for spatially distributed infrastructure: {HAZARD} {ANALYSIS} {FOR} {SPATIALLY} {DISTRIBUTED} {INFRASTRUCTURE}},
	issn = {00988847},
	shorttitle = {Probabilistic seismic hazard analysis for spatially distributed infrastructure},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/eqe.2179},
	doi = {10.1002/eqe.2179},
	abstract = {Two key issues distinguish probabilistic seismic risk analysis of a lifeline or portfolio of structures from that of a single structure. Regional analysis must consider the correlation among lifeline components or structures in the portfolio, and the larger scope makes it much more computationally demanding. In this paper, we systematically identify and compare alternative methods for regional hazard analysis that can be used as the ﬁrst part of a computationally efﬁcient regional probabilistic seismic risk analysis that properly considers spatial correlation. Speciﬁcally, each method results in a set of probabilistic ground motion maps with associated hazard-consistent annual occurrence probabilities that together represent the regional hazard. The methods are compared according to how replicable and computationally tractable they are and the extent to which the resulting maps are physically realistic, consistent with the regional hazard and regional spatial correlation, and few in number. On the basis of a conceptual comparison and an empirical comparison for Los Angeles, we recommend a combination of simulation and optimization approaches: (i) Monte Carlo simulation with importance sampling of the earthquake magnitudes to generate a set of probabilistic earthquake scenarios (deﬁned by source and magnitude); (ii) the optimization-based probabilistic scenario method, a mixed-integer linear program, to reduce the size of that set; (iii) Monte Carlo simulation to generate a set of probabilistic ground motion maps, varying the number of maps sampled from each earthquake scenario so as to minimize the sampling variance; and (iv) the optimization-based probabilistic scenario again to reduce the set of probabilistic ground motion maps. Copyright © 2012 John Wiley \& Sons, Ltd.},
	language = {en},
	urldate = {2022-06-29},
	journal = {Earthquake Engineering \& Structural Dynamics},
	author = {Han, Yeliang and Davidson, Rachel A.},
	month = mar,
	year = {2012},
	pages = {n/a--n/a},
}

@article{jayaram_efficient_2010,
	title = {Efficient sampling and data reduction techniques for probabilistic seismic lifeline risk assessment},
	issn = {00988847, 10969845},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/eqe.988},
	doi = {10.1002/eqe.988},
	abstract = {Probabilistic seismic risk assessment for spatially distributed lifelines is less straightforward than for individual structures. While procedures such as the ‘PEER framework’ have been developed for risk assessment of individual structures, these are not easily applicable to distributed lifeline systems, due to difﬁculties in describing ground-motion intensity (e.g. spectral acceleration) over a region (in contrast to ground-motion intensity at a single site, which is easily quantiﬁed using Probabilistic Seismic Hazard Analysis), and since the link between the ground-motion intensities and lifeline performance is usually not available in closed form. As a result, Monte Carlo simulation (MCS) and its variants are well suited for characterizing ground motions and computing resulting losses to lifelines. This paper proposes a simulation-based framework for developing a small but stochastically representative catalog of earthquake ground-motion intensity maps that can be used for lifeline risk assessment. In this framework, Importance Sampling is used to preferentially sample ‘important’ ground-motion intensity maps, and K -Means Clustering is used to identify and combine redundant maps in order to obtain a small catalog. The effects of sampling and clustering are accounted for through a weighting on each remaining map, so that the resulting catalog is still a probabilistically correct representation. The feasibility of the proposed simulation framework is illustrated by using it to assess the seismic risk of a simpliﬁed model of the San Francisco Bay Area transportation network. A catalog of just 150 intensity maps is generated to represent hazard at 1038 sites from 10 regional fault segments causing earthquakes with magnitudes between ﬁve and eight. The risk estimates obtained using these maps are consistent with those obtained using conventional MCS utilizing many orders of magnitudes more ground-motion intensity maps. Therefore, the proposed technique can be used to drastically reduce the computational expense of a simulationbased risk assessment, without compromising the accuracy of the risk estimates. This will facilitate computationally intensive risk analysis of systems such as transportation networks. Finally, the study shows that the uncertainties in the ground-motion intensities and the spatial correlations between ground-motion intensities at various sites must be modeled in order to obtain unbiased estimates of lifeline risk. Copyright q 2010 John Wiley \& Sons, Ltd.},
	language = {en},
	urldate = {2022-06-29},
	journal = {Earthquake Engineering \& Structural Dynamics},
	author = {Jayaram, Nirmal and Baker, Jack W.},
	year = {2010},
	pages = {n/a--n/a},
}

@article{mahadevan_uncertainty_nodate,
	title = {{UNCERTAINTY} {ANALYSIS} {METHODS}},
	language = {en},
	author = {Mahadevan, Sankaran and Sarkar, Sohini},
	pages = {32},
}

@article{fragiadakis_seismic_2014,
	title = {Seismic reliability assessment of urban water networks},
	volume = {43},
	issn = {1096-9845},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/eqe.2348},
	doi = {10.1002/eqe.2348},
	abstract = {SUMMARYWe present a framework for the seismic risk assessment of water supply networks, operating in either normal or abnormal conditions. We propose a methodology for assessing the reliability of water pipe networks combining data of past non-seismic damage and the vulnerability of the network components against seismic loading. Historical data are obtained using records of damages that occur on a daily basis throughout the network and are processed to produce‘survival curves’, depicting their estimated survival rate over time. The fragility of the network components is assessed using the approach suggested in the American Lifelines Alliance guidelines. The network reliability is assessed using graph theory, whereas the system network reliability is calculated using Monte Carlo simulation. The methodology proposed is demonstrated both on a simple, small-scale, network and also on a real-scale district metered area from the water network of the city of Limassol, Cyprus. The proposed approach allows the estimation of the probability that the network fails to provide the desired level of service and allows the prioritization of retrofit interventions and of capacity-upgrade actions pertaining to existing water pipe networks. Copyright © 2013 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {3},
	urldate = {2022-11-08},
	journal = {Earthquake Engineering \& Structural Dynamics},
	author = {Fragiadakis, Michalis and Christodoulou, Symeon E.},
	year = {2014},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/eqe.2348},
	keywords = {graph theory, lifelines, reliability assessment, survival analysis, water distribution networks (WDNs)},
	pages = {357--374},
}

@techreport{american_lifelines_alliance_ala_seismic_2005,
	title = {Seismic guidelines for water pipelines},
	institution = {ALA (American Lifelines Alliance)},
	author = {American Lifelines Alliance (ALA)},
	month = mar,
	year = {2005},
}

@techreport{american_lifelines_alliance_ala_seismic_2001,
	title = {Seismic fragility formulations for water systems-guideline and appendices},
	institution = {ALA (American Lifelines Alliance)},
	author = {American Lifelines Alliance (ALA)},
	month = apr,
	year = {2001},
}

@article{hwang_seismic_1998,
	title = {Seismic performance assessment of water delivery systems},
	volume = {4},
	language = {English},
	number = {3},
	journal = {Journal of Infrastructure Systems},
	author = {Hwang, Howard HM and Huijie, Lin and Masanobu, Shinozuka},
	year = {1998},
	pages = {118--125},
}

@article{kleiner_long-term_1998,
	title = {Long-term planning methodology for water distribution system rehabilitation},
	volume = {34},
	issn = {1944-7973},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1029/98WR00377},
	doi = {10.1029/98WR00377},
	abstract = {The most expensive component of a water supply system is the distribution network. Deterioration due to aging and stress causes increased operation and maintenance costs, water losses, reduction in the quality of service, and reduction in the quality of water supplied. In this paper an approach is proposed in which the water distribution network economics and hydraulic capacity are analyzed simultaneously over a predefined analysis period while the deterioration over time of both the structural integrity and the hydraulic capacity of every pipe in the system is explicitly considered. The cost associated with each pipe in the network is calculated as the present value of an infinite stream of costs. In Kleiner et al. [this issue] a methodology is presented to implement this approach into a decision support system that facilitates the identification of an optimal rehabilitation strategy.},
	language = {en},
	number = {8},
	urldate = {2022-11-07},
	journal = {Water Resources Research},
	author = {Kleiner, Yehuda and Adams, Barry J. and Rogers, J. Scott},
	year = {1998},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1029/98WR00377},
	pages = {2039--2051},
}

@article{haario_adaptive_2001,
	title = {An {Adaptive} {Metropolis} {Algorithm}},
	volume = {7},
	issn = {13507265},
	url = {https://www.jstor.org/stable/3318737?origin=crossref},
	doi = {10.2307/3318737},
	language = {en},
	number = {2},
	urldate = {2022-11-01},
	journal = {Bernoulli},
	author = {Haario, Heikki and Saksman, Eero and Tamminen, Johanna},
	month = apr,
	year = {2001},
	pages = {223},
}

@article{sun_resilience_2020,
	title = {Resilience metrics and measurement methods for transportation infrastructure: the state of the art},
	volume = {5},
	issn = {2378-9689, 2378-9697},
	shorttitle = {Resilience metrics and measurement methods for transportation infrastructure},
	url = {https://www.tandfonline.com/doi/full/10.1080/23789689.2018.1448663},
	doi = {10.1080/23789689.2018.1448663},
	abstract = {The transportation infrastructure plays an important role in supporting the national economy and ensuring the well-being of its citizenry. Extreme events (including both natural hazards and manmade disasters) have caused terrible physical damages to the transportation infrastructure, longterm socioeconomic impacts, and psychological damages. There is an increasing number of studies focusing on the resilience analysis of the transportation infrastructure to support planning and design and to optimize emergency management and restoration schedules. We provide a comprehensive review of recent literature on the metrics and methods for resilience analyses of the transportation infrastructure under extreme events. The paper covers functionality metrics, functionality-based resilience metrics and socio-economic resilience metrics. Despite the advancements of research in resilience, there are still fundamental challenges to comprehensively evaluate the resilience of the transportation infrastructure, especially due to two main sources of complexity: uncertainties and interdependencies, which are discussed in the paper. We also point out that validations of resilience assessments are limited due to the general scarcity of data, which may hinder the practical applications. Finally, directions for future research in this field are suggested. The paper provides an organized overview of the many lines of research in the field, accomplishments, and open gaps. It indicates useful starting points for researchers new to this field, and serves as a reference for teams already active on this topic.},
	language = {English},
	number = {3},
	urldate = {2022-10-03},
	journal = {Sustainable and Resilient Infrastructure},
	author = {Sun, Wenjuan and Bocchini, Paolo and Davison, Brian D.},
	month = may,
	year = {2020},
	pages = {168--199},
}

@article{lounis_risk-based_2016,
	title = {Risk-{Based} {Decision} {Making} for {Sustainable} and {Resilient} {Infrastructure} {Systems}},
	volume = {142},
	doi = {10.1061/(ASCE)ST.1943-541X.0001545},
	abstract = {The development of infrastructure systems that are sustainable and resilient is a challenging task that involves a broad range of performance indicators over the system lifecycle that affect system functionality and recovery. Sustainability indicators address economic, social, and environmental performance metrics and resilient indicators address strength, functionality, and recovery-time metrics following a hazard event. Sustainable systems consider environmental impact and conservation of nonrenewable resources over the life of the system. Resilient systems consider performance levels relative to potential damage levels and recovery times from events. Both concepts address adequate system performance and lifecycle costs, but put a different emphasis on other indicators. Numerous sources of uncertainties associated with the lifecycle performance of infrastructure systems require the use of a risk-informed decision-making approach to properly account for uncertainties and to identify cost-effective strategies to manage risk. A framework for risk-informed decision making for the lifecycle performance of infrastructure facilities that includes consideration of sustainability and resilience is presented. Separate examples are provided for the same highway bridge deck system to illustrate sustainable and resilient performance objectives with the design and rehabilitation of highway bridge decks. The sustainability assessment considers the effect of corrosion degradation mechanisms on lifecycle costs, environmental impact (CO2 and waste), and social impacts (accidents and user time) while maintaining service life and structural safety. The resilience assessment considers the effect of seismic hazard events on structural damage levels and recovery time while maintaining system functionality and structural safety.},
	number = {9},
	journal = {Journal of Structural Engineering},
	author = {Lounis, Zoubir and Mcallister, Therese},
	month = jun,
	year = {2016},
	pages = {F4016005},
}

@article{aven_role_2009,
	title = {The {Role} of {Quantitative} {Risk} {Assessments} for {Characterizing} {Risk} and {Uncertainty} and {Delineating} {Appropriate} {Risk} {Management} {Options}, with {Special} {Emphasis} on {Terrorism} {Risk}},
	volume = {29},
	issn = {02724332, 15396924},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1539-6924.2008.01175.x},
	doi = {10.1111/j.1539-6924.2008.01175.x},
	language = {English},
	number = {4},
	urldate = {2022-10-03},
	journal = {Risk Analysis},
	author = {Aven, Terje and Renn, Ortwin},
	month = apr,
	year = {2009},
	pages = {587--600},
}

@article{tung_uncertainty_2011,
	title = {Uncertainty and {Reliability} {Analysis} in {Water} {Resources} {Engineering}},
	volume = {103},
	language = {English},
	number = {1},
	journal = {Journal of Contemporary Water Research and Education},
	author = {Tung, Yeou-Koung},
	year = {2011},
	pages = {9},
}

@techreport{hayes_uncertainty_2011,
	title = {Uncertainty and {Uncertainty} {Analysis} {Methods}},
	number = {EP102467},
	author = {Hayes, Keith R.},
	year = {2011},
}

@book{morgan_uncertainty_1992,
	title = {Uncertainty: {A} {Guide} to {Dealing} with {Uncertainty} in {Quantitative} {Risk} and {Policy} {Analysis}},
	isbn = {978-0-521-42744-9},
	shorttitle = {Uncertainty},
	abstract = {The authors explain the ways in which uncertainty is an important factor in the problems of risk and policy analysis. This book outlines the source and nature of uncertainty, discusses techniques for obtaining and using expert judgment, and reviews a variety of simple and advanced methods for analyzing uncertainty. Powerful computer environments and good graphical techniques for displaying uncertainty are just two of the more advanced topics addressed in later chapters.},
	language = {English},
	publisher = {Cambridge University Press},
	author = {Morgan, Millett Granger and Henrion, Max and Small, Mitchell},
	month = jun,
	year = {1992},
	keywords = {Business \& Economics / Entrepreneurship, Business \& Economics / General, Business \& Economics / Management, Psychology / Cognitive Psychology \& Cognition},
}

@misc{committee_guidance_2018,
	title = {Guidance on {Uncertainty} {Analysis} in {Scientific} {Assessments}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.2903/j.efsa.2018.5123},
	abstract = {Uncertainty analysis is the process of identifying limitations in scientific knowledge and evaluating their implications for scientific conclusions. It is therefore relevant in all EFSA's scientific assessments and also necessary, to ensure that the assessment conclusions provide reliable information for decision-making. The form and extent of uncertainty analysis, and how the conclusions should be reported, vary widely depending on the nature and context of each assessment and the degree of uncertainty that is present. This document provides concise guidance on how to identify which options for uncertainty analysis are appropriate in each assessment, and how to apply them. It is accompanied by a separate, supporting opinion that explains the key concepts and principles behind this Guidance, and describes the methods in more detail.},
	language = {English},
	urldate = {2022-10-03},
	author = {Committee, EFSA Scientific and Benford, Diane and Halldorsson, Thorhallur and Jeger, Michael John and Knutsen, Helle Katrine and More, Simon and Naegeli, Hanspeter and Noteborn, Hubert and Ockleford, Colin and Ricci, Antonia and Rychen, Guido and Schlatter, Josef R and Silano, Vittorio and Solecki, Roland and Turck, Dominique and Younes, Maged and Craig, Peter and Hart, Andrew and Von Goetz, Natalie and Koutsoumanis, Kostas and Mortensen, Alicja and Ossendorp, Bernadette and Martino, Laura and Merten, Caroline and Mosbach-Schulz, Olaf and Hardy, Anthony},
	year = {2018},
	keywords = {guidance, scientific assessment, uncertainty analysis},
}

@book{gelman_handbook_2010,
	title = {Handbook of {Markov} {Chain} {Monte} {Carlo}: {Methods} and {Applications}},
	publisher = {Chapman \& Hall/CRC},
	author = {Gelman, A and Brooks, S and Jones, G and Meng, XL},
	year = {2010},
}

@phdthesis{padgett_seismic_nodate,
	title = {Seismic {Vulnerability} {Assessment} of {Retrofitted} {Bridges} {Using} {Probabilistic} {Methods}},
	author = {Padgett, Jamie Ellen},
}

@article{li_surrogate_2019,
	title = {Surrogate model uncertainty quantification for reliability-based design optimization},
	volume = {192},
	issn = {09518320},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0951832018305611},
	doi = {10.1016/j.ress.2019.03.039},
	abstract = {Surrogate models have been widely employed as approximations of expensive physics-based simulations to alleviate the computational burden in reliability-based design optimization. Ignoring the surrogate model uncertainty due to the lack of training samples will lead to untrustworthy designs in product development. This paper addresses the surrogate model uncertainty in reliability analysis using the equivalent reliability index (ERI) and further develops a new smooth sensitivity analysis approach to facilitate the surrogate model-based product design process. By using the Gaussian process (GP) modeling technique, a Gaussian mixture model (GMM) is constructed for reliability analysis using Monte Carlo simulations. To propagate both input variations and surrogate model uncertainty, the probability of failure is approximated by calculating the equivalent reliability index using the first and second statistical moments of the GMM. The sensitivity of ERI with respect to design variables is analytically derived based on the GP predictions. Three case studies are used to demonstrate the effectiveness and robustness of the proposed approach.},
	language = {en},
	urldate = {2022-10-26},
	journal = {Reliability Engineering \& System Safety},
	author = {Li, Mingyang and Wang, Zequn},
	month = dec,
	year = {2019},
	pages = {106432},
}

@inproceedings{rokneddin_uncertainty_2015,
	address = {Vancouver, Canada},
	title = {Uncertainty {Propagation} in {Seismic} {Reliability} {Evaluation} of {Aging} {Transportation} {Networks}},
	abstract = {Uncertainty quantification is an integral part of many fields of science and engineering, but its application to seismic reliability and risk assessment in highway transportation networks is still in its infancy. This study identifies major known sources of uncertainties associated with seismic loss assessments in aging transportation networks, including hazards, structures, aging parameters, and network topology sources, while quantifying the impact of a subset of them on mean network-level reliability estimates. The uncertainty tracking process is illustrated with a case study network in South Carolina, USA. The source-to-response uncertainties are propagated and errors aggregated as they emerge with the adoption of surrogate seismic response models at both bridge and network levels. The observed range of uncertainties from the considered sources suggests that uncertainty quantification must become a standard procedure for reliability and risk assessment in transportation networks. Moreover, while the bridge surrogate models contribute significantly to overall uncertainties, network surrogate model's contribution is found to be the least of all considered variables. Future opportunities exist to further identify key sources that should be targeted for improved confidence in risk estimates.},
	author = {Rokneddin, Keivan and Ghosh, Jayadipta and Dueñas-Osorio, Leonardo and Padgett, Jamie},
	month = jul,
	year = {2015},
}

@article{ren_assessing_2020,
	title = {Assessing the reliability, resilience and vulnerability of water supply system under multiple uncertain sources},
	volume = {252},
	issn = {09596526},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0959652619346761},
	doi = {10.1016/j.jclepro.2019.119806},
	abstract = {Water reservoirs often contain multiple runoff water sources, each of which is affected by a variety of uncertainties. The encounter situation deﬁned in multiple water sources is the imbalance spatialtemporal distribution of water resources, which has a profound impact on the risk of water scarcity. This study presents a framework to evaluate the performance of a water supply system considering the encounter situations between different water sources. Based on the Han to Wei Inter-Basin Water Transfer (IBWT) project in the Shaanxi Province of China, the synchronous and asynchronous encounter probabilities are calculated using copula-based approach to characterize the encounter risks for water supply. Then, the synthetic monthly series of different water sources are generated through a synthetic streamﬂow-generated method based on the simulated annealing algorithm and fragment method, which is applied as the input variables of a reservoir optimization operation model. The metrics of reliability, resilience, and vulnerability are used to evaluate the performance of the water supply system. Results indicate that (1) regardless of whether the annual ﬂow series is a full sequence or a pre- or post-changed sequence, the encounter probabilities between different water sources remain stable. However, there are obvious changes in the encounter probabilities of each month between different water sources; (2) The generated monthly ﬂow series effectively preserves the internal composition of the historical data, which are adequate to evaluate the performance of the water supply system; (3) as the water demand increases, the reservoir’s storage capacity decreases, and the reliability of the water supply depends on the wet encounter situations for different water sources; (4) the ability to describe the states of the system varies signiﬁcantly according to the metrics selected, and the choice of the metrics of reliability, resilience (max), and vulnerability (mean) can adequately describe the states of the water supply system. This study appropriately constrained the uncertain inputs of the reservoir operation model to improve the credibility of decision making.},
	language = {en},
	urldate = {2022-10-26},
	journal = {Journal of Cleaner Production},
	author = {Ren, Kang and Huang, Shengzhi and Huang, Qiang and Wang, Hao and Leng, Guoyong and Fang, Wei and Li, Pei},
	month = apr,
	year = {2020},
	pages = {119806},
}

@book{reed_addressing_nodate,
	title = {Addressing {Uncertainty} in {MultiSector} {Dynamics} {Research}},
	language = {English},
	author = {Reed, Patrick M and Hadjimichael, Antonia and Malek, Keyvan and Karimi, Tina and Vernon, Chris R and Srikrishnan, Vivek and Gupta, Rohini S and Gold, David F and Lee, Ben and Keller, Klaus and Thurber, Travis B and Rice, Jennie S},
}

@article{stavroulakis_new_2014,
	title = {A new perspective on the solution of uncertainty quantification and reliability analysis of large-scale problems},
	volume = {276},
	issn = {00457825},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0045782514000954},
	doi = {10.1016/j.cma.2014.03.009},
	abstract = {This work revisits the computational performance of non-intrusive Monte Carlo versus intrusive Galerkin methods of large-scale stochastic systems in the framework of high performance computing environments. The purpose of this work is to perform an assessment of the range of the relative superiority of these approaches with regard to a variety of stochastic parameters. In both approaches, the solution of the resulting algebraic equations is performed with a combination of primal and dual domain decomposition methods implementing speciﬁcally tailored preconditioners. The solution of repeated simulations of the Monte Carlo method is accelerated with an A-orthogonalization procedure aiming at reducing the iterations of subsequent simulations, while the solution of the augmented equations of the stochastic Galerkin method is enhanced with preconditioners which combine the block diagonal features of the resulting matrices as well as the sparsity pattern of the oﬀ block-diagonal terms. Numerical results are presented, demonstrating the eﬃciency of the proposed implementations on a large-scale 3D problem with diﬀerent stochastic characteristics and useful conclusions are derived regarding the ranges of stochastic parameters in which non-intrusive solvers have a superior performance compared to intrusive ones and vice versa.},
	language = {en},
	urldate = {2022-10-26},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Stavroulakis, George and Giovanis, Dimitris G. and Papadrakakis, Manolis and Papadopoulos, Vissarion},
	month = jul,
	year = {2014},
	pages = {627--658},
}

@article{cox_methods_1981,
	title = {Methods for {Uncertainty} {Analysis}: {A} {Comparative} {Survey}},
	volume = {1},
	issn = {0272-4332, 1539-6924},
	shorttitle = {Methods for {Uncertainty} {Analysis}},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1539-6924.1981.tb01425.x},
	doi = {10.1111/j.1539-6924.1981.tb01425.x},
	language = {en},
	number = {4},
	urldate = {2022-10-26},
	journal = {Risk Analysis},
	author = {Cox, David C. and Baybutt, Paul},
	month = dec,
	year = {1981},
	pages = {251--258},
}

@article{kiureghian_aleatory_2009,
	title = {Aleatory or epistemic? {Does} it matter?},
	volume = {31},
	issn = {01674730},
	shorttitle = {Aleatory or epistemic?},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167473008000556},
	doi = {10.1016/j.strusafe.2008.06.020},
	abstract = {The sources and characters of uncertainties in engineering modeling for risk and reliability analyses are discussed. While many sources of uncertainty may exist, they are generally categorized as either aleatory or epistemic. Uncertainties are characterized as epistemic, if the modeler sees a possibility to reduce them by gathering more data or by reﬁning models. Uncertainties are categorized as aleatory if the modeler does not foresee the possibility of reducing them. From a pragmatic standpoint, it is useful to thus categorize the uncertainties within a model, since it then becomes clear as to which uncertainties have the potential of being reduced. More importantly, epistemic uncertainties may introduce dependence among random events, which may not be properly noted if the character of uncertainties is not correctly modeled. Inﬂuences of the two types of uncertainties in reliability assessment, codiﬁed design, performance-based engineering and risk-based decision-making are discussed. Two simple examples demonstrate the inﬂuence of statistical dependence arising from epistemic uncertainties on systems and time-variant reliability problems.},
	language = {en},
	number = {2},
	urldate = {2022-10-14},
	journal = {Structural Safety},
	author = {Kiureghian, Armen Der and Ditlevsen, Ove},
	month = mar,
	year = {2009},
	pages = {105--112},
}

@inproceedings{nazemi_multivariate_2020,
	title = {Multivariate {Uncertainty} {Characterization} for {Resilience} {Planning} in {Electric} {Power} {Systems}},
	doi = {10.1109/ICPS48389.2020.9176794},
	abstract = {Following substantial advancements in stochastic classes of decision-making optimization problems, scenario-based stochastic optimization, robust{\textbackslash} distributionally robust optimization, and chance-constrained optimization have recently gained an increasing attention. Despite the remarkable developments in probabilistic forecast of uncertainties (e.g., in renewable energies), most approaches are still being employed in a univariate framework which fails to unlock a full understanding on the underlying interdependence among uncertain variables of interest. In order to yield cost-optimal solutions with predefined probabilistic guarantees, conditional and dynamic interdependence in uncertainty forecasts should be accommodated in power systems decision-making. This becomes even more important during the emergencies where high-impact low-probability (HILP) disasters result in remarkable fluctuations in the uncertain variables. In order to model the interdependence correlation structure between different sources of uncertainty in power systems during both normal and emergency operating conditions, this paper aims to bridge the gap between the probabilistic forecasting methods and advanced optimization paradigms; in particular, perdition regions are generated in the form of ellipsoids with probabilistic guarantees. We employ a modified Khachiyan's algorithm to compute the minimum volume enclosing ellipsoids (MVEE). Application results based on two datasets on wind and photovoltaic power are used to verify the efficiency of the proposed framework.},
	booktitle = {2020 {IEEE}/{IAS} 56th {Industrial} and {Commercial} {Power} {Systems} {Technical} {Conference} ({I}\&{CPS})},
	author = {Nazemi, Mostafa and Dehghanian, Payman and Alhazmi, Mohannad and Wang, Fei},
	month = jun,
	year = {2020},
	note = {ISSN: 2158-4907},
	keywords = {Decision making, Ellipsoids, Optimization, Power systems, Probabilistic forecasting, Probabilistic logic, Stochastic processes, Uncertainty, ellipsoids, resilience, stochastic optimization, uncertainty sets},
	pages = {1--8},
}

@article{allen_quantifying_2018,
	title = {Quantifying uncertainty and trade-offs in resilience assessments},
	volume = {23},
	issn = {1708-3087},
	url = {https://www.ecologyandsociety.org/vol23/iss1/art3/},
	doi = {10.5751/ES-09920-230103},
	abstract = {Several frameworks have been developed to assess the resilience of social-ecological systems, but most require substantial data inputs, time, and technical expertise. Stakeholders and practitioners often lack the resources for such intensive efforts. Furthermore, most end with problem framing and fail to explicitly address trade-offs and uncertainty. To remedy this gap, we developed a rapid survey assessment that compares the relative resilience of social-ecological systems with respect to a number of resilience properties. This approach generates large amounts of information relative to stakeholder inputs. We targeted four stakeholder categories: government (policy, regulation, management), end users (farmers, ranchers, landowners, industry), agency/public science (research, university, extension), and NGOs (environmental, citizen, social justice) in four North American watersheds, to assess social-ecological resilience through surveys. Conceptually, social-ecological systems are comprised of components ranging from strictly human to strictly ecological, but that relate directly or indirectly to one another. They have soft boundaries and several important dimensions or axes that together describe the nature of social-ecological interactions, e.g., variability, diversity, modularity, slow variables, feedbacks, capital, innovation, redundancy, and ecosystem services. There is no absolute measure of resilience, so our design takes advantage of cross-watershed comparisons and therefore focuses on relative resilience. Our approach quantifies and compares the relative resilience across watershed systems and potential trade-offs among different aspects of the social-ecological system, e.g., between social, economic, and ecological contributions. This approach permits explicit assessment of several types of uncertainty (e.g., self-assigned uncertainty for stakeholders; uncertainty across respondents, watersheds, and subsystems), and subjectivity in perceptions of resilience among key actors and decision makers and provides an efficient way to develop the mental models that inform our stakeholders and stakeholder categories.},
	language = {en},
	number = {1},
	urldate = {2022-10-06},
	journal = {Ecology and Society},
	author = {Allen, Craig R. and Birge, Hannah E. and Angeler, David G. and Arnold, Craig Anthony (Tony) and Chaffin, Brian C. and DeCaro, Daniel A. and Garmestani, Ahjond S. and Gunderson, Lance},
	year = {2018},
	pages = {art3},
}

@book{murray_import_2002,
	title = {Import risk analysis: animals and animal products.},
	shorttitle = {Import risk analysis},
	url = {https://www.cabdirect.org/cabdirect/abstract/20023116178},
	abstract = {This book contains 12 chapters. Topics covered are: import risks analysis for animals and animal products; managing a risk analysis project; references, editorial guidelines and terminology; applying the OIE risk analysis framework; introducing quantitative risk assessment; probability and probability distribution; theorems underpining probabilistic analysis; useful probability distributors;...},
	language = {English},
	urldate = {2022-10-05},
	publisher = {New Zealand Ministry of Agriculture and Forestry},
	author = {Murray, N.},
	year = {2002},
}

@book{cullen_probabilistic_1999,
	title = {Probabilistic {Techniques} in {Exposure} {Assessment}: {A} {Handbook} for {Dealing} with {Variability} and {Uncertainty} in {Models} and {Inputs}},
	isbn = {978-0-306-45956-6},
	shorttitle = {Probabilistic {Techniques} in {Exposure} {Assessment}},
	abstract = {At this time when regulatory agencies are accepting and actively encouraging probabilistic approaches and the attribution of overall uncertainty among inputs to support Value of Information analyses, a comprehensive sourcebook on methods for addressing variability and uncertainty in exposure analysis is sorely needed. This need is adroitly met in Probabilistic Techniques in Exposure Assessment. A host of expert contributors provide a straightforward introduction to the practical tools for addressing variability and uncertainty in support of environmental and human health decision making. 151 graphs, plots, charts, and figures supplement a broad range of detailed and practical examples.},
	language = {en},
	publisher = {Springer Science \& Business},
	author = {Cullen, Alison C. and Frey, H. Christopher and Frey, Christopher H.},
	year = {1999},
	note = {Google-Books-ID: P9l5q4RjcwcC},
	keywords = {Medical / Public Health, Nature / Environmental Conservation \& Protection, Nature / Natural Resources, Nature / Reference, Science / Environmental Science, Technology \& Engineering / Environmental / General},
}

@book{beven_environmental_nodate,
	title = {Environmental {Modelling}: {An} {Uncertain} {Future}?},
	language = {en},
	publisher = {Taylor and Francis Group},
	author = {Beven, Keith},
}

@book{kochenderfer_decision_2015,
	address = {Cambridge, Massachusetts},
	series = {Lincoln {Laboratory} series},
	title = {Decision making under uncertainty: theory and application},
	isbn = {978-0-262-02925-4},
	shorttitle = {Decision making under uncertainty},
	language = {en},
	publisher = {The MIT Press},
	author = {Kochenderfer, Mykel J.},
	year = {2015},
	keywords = {Automatic machinery, Decision making, Intelligent control systems, Mathematical models},
}
